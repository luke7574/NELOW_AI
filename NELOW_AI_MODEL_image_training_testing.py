import csv
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import keras
from keras import backend as K
import numpy as np
import cv2
import os
from tqdm import tqdm  # tqdm ì¶”ê°€
import librosa
from scipy.fftpack import fft


# # ê²½ë¡œ ì„¤ì •
AI_model_training = 'Graph_Image/AI_Model/NELOW_GL_img_model_2.h5'
AI_model_testing = 'Graph_Image/AI_Model/NELOW_GL_img_model_2.h5'

# WAV_files_path_training = 'Training_SWM/WAV_files/'
# Numpy_files_path_training = 'Training_SWM/Numpy_files/'
#
# âœ… ì˜ˆì¸¡í•  ë°ì´í„° í´ë”
wav_root_dir = "C:/Users/gram/AI/NELOW/NELOW_AI/Graph_Image/predict_WAV_files/"
output_data_dir = "C:/Users/gram/AI/NELOW/NELOW_AI/Graph_Image/predict_data/"
csv_output_path = "C:/Users/gram/AI/NELOW/NELOW_AI/Graph_Image/predict_result/"

#í•™ìŠµ ê·¸ë˜í”„ ê²½ë¡œ/íŒŒì¼ëª… ì„¤ì •
train_plot = 'C:/Users/gram/AI/NELOW/NELOW_AI/Graph_Image/plot_history/NELOW_GL_img_model_3.png'

model_training = 0

model_testing = 1

# ì´ë¯¸ì§€ í¬ê¸° ì„¤ì •
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 100
CLASSES = 3  # 3ê°€ì§€ í´ë˜ìŠ¤

# Define recall metric
def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

# Define precision metric
def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

# Define F1 score metric
def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# 1ï¸âƒ£ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì´ë¯¸ì§€ 2ê°œë¥¼ 1ìŒìœ¼ë¡œ ë¬¶ìŒ)
def load_image_pair(image_path1, image_path2):
    img1 = cv2.imread(image_path1)
    img2 = cv2.imread(image_path2)

    img1 = cv2.resize(img1, IMG_SIZE) / 255.0  # í¬ê¸° ë³€í™˜ + ì •ê·œí™”
    img2 = cv2.resize(img2, IMG_SIZE) / 255.0  # í¬ê¸° ë³€í™˜ + ì •ê·œí™”

    return np.array(img1), np.array(img2)

# 2ï¸âƒ£ CNN ëª¨ë¸ ì •ì˜ (2ê°œì˜ ì´ë¯¸ì§€ ì…ë ¥ â†’ 3ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜)
def build_dual_input_model():
    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ CNN ë„¤íŠ¸ì›Œí¬
    input1 = Input(shape=(128, 128, 3), name="image_1")
    x1 = Conv2D(32, (3, 3), activation="relu", padding="same")(input1)
    x1 = MaxPooling2D(pool_size=(2, 2))(x1)
    x1 = Flatten()(x1)

    # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ CNN ë„¤íŠ¸ì›Œí¬
    input2 = Input(shape=(128, 128, 3), name="image_2")
    x2 = Conv2D(32, (3, 3), activation="relu", padding="same")(input2)
    x2 = MaxPooling2D(pool_size=(2, 2))(x2)
    x2 = Flatten()(x2)

    # ë‘ ê°œì˜ íŠ¹ì§• ë²¡í„°ë¥¼ ê²°í•©
    merged = Concatenate()([x1, x2])
    x = Dense(64, activation="relu")(merged)
    output = Dense(CLASSES, activation="softmax")(x)  # 3ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜

    model = Model(inputs=[input1, input2], outputs=output)
    model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam',
                  metrics=['accuracy', f1_m, precision_m, recall_m])
    return model

def get_NELOW_values(wav_path):
    s_fft = []
    data, samplerate = librosa.load(wav_path, sr=None, duration=5)

    SEC_0_1 = int(samplerate / 10)
    SEC_1 = samplerate

    time_l = int(len(data) / SEC_1)
    i_time = (time_l - 1) * 10 - 1

    for i in range(i_time):
        u_data = abs(data[(i + 1) * SEC_0_1:(i + 1) * SEC_0_1 + SEC_1])
        s_fft.append(np.std(u_data))
    a = np.argmin(s_fft) + 1 # í‘œì¤€í¸ì°¨ê°€ ê°€ì¥ ì‘ì€ 1ì´ˆ êµ¬ê°„ ì„ íƒ

    tfa_data = abs(fft(data[a * SEC_0_1:a * SEC_0_1 + SEC_1])) # ì‹œê°„ ë„ë©”ì¸ ë°ì´í„°(tfa_data)ë¥¼ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜
    tfa_data3000 = tfa_data[0:3000]
    tfa_data3000[:50] = 0

    idx = np.argmax(tfa_data3000) # ê°€ì¥ í° ì£¼íŒŒìˆ˜ ì„±ë¶„ì˜ ì¸ë±ìŠ¤(ì£¼íŒŒìˆ˜ ìœ„ì¹˜)

    startPos = 0

    if idx < 10:
        startPos = 0
    else:
        startPos = idx - 10
    stopPos = idx + 10

    # jsoníŒŒì¼
    NELOW_fft_data = tfa_data3000.tolist()  # 0~3000Hzê¹Œì§€ì˜ FFT ë³€í™˜ ë°ì´í„° (ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼)
    # í‘œì¤€í¸ì°¨
    std_deviation = np.std(tfa_data)        # í‘œì¤€í¸ì°¨ (ì†Œë¦¬ì˜ ë³€ë™ ì •ë„)
    # ëˆ„ìˆ˜ê°•ë„
    wave_energy = np.average(tfa_data3000[startPos:stopPos])   # ê°€ì¥ ê°•í•œ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì˜ í‰ê·  ì§„í­ (ëˆ„ìˆ˜ ê°•ë„)
    # ìµœëŒ€ì£¼íŒŒìˆ˜
    wave_max_frequency = np.argmax(tfa_data3000)               # ìµœëŒ€ ì£¼íŒŒìˆ˜ ì„±ë¶„ì˜ ìœ„ì¹˜(ì¸ë±ìŠ¤ê°’) (ì£¼íŒŒìˆ˜ Hz ë‹¨ìœ„)

    return NELOW_fft_data, std_deviation, wave_energy, wave_max_frequency

# âœ… WAV íŒŒì¼ì˜ Waveform ë° Spectrum ì €ì¥ í•¨ìˆ˜
def save_graphs(file_path, output_folder):
    file_name = os.path.splitext(os.path.basename(file_path))[0]

    # âœ… ì‹¤ì œ í´ë˜ìŠ¤ ìë™ ì¶”ì¶œ (íŒŒì¼ëª… ê¸°ì¤€)
    actual_class = "unknown"
    if "_L" in file_name:
        actual_class = "leak"
    elif "_M" in file_name:
        actual_class = "meter"
    elif "_N" in file_name:
        actual_class = "no_leak"

    # âœ… ì €ì¥ í´ë” ì„¤ì • (í´ë˜ìŠ¤ë³„ í´ë”)
    save_dir = os.path.join(output_folder, actual_class)
    os.makedirs(save_dir, exist_ok=True)

    # âœ… íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    waveform_path = os.path.join(save_dir, f"{file_name}_waveform.png")
    spectrum_path = os.path.join(save_dir, f"{file_name}_spectrum.png")

    # âœ… ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
    raw_y, raw_sr = librosa.load(file_path, sr=None)
    NELOW_fft_data, _, wave_energy, wave_max_frequency = get_NELOW_values(file_path)

    # âœ… íŒŒí˜•(Waveform) ê·¸ë˜í”„ ì €ì¥
    fig_wave, ax1 = plt.subplots(figsize=(10, 3))
    librosa.display.waveshow(raw_y, sr=raw_sr, ax=ax1)
    ax1.set_ylim(-1, 1)
    fig_wave.savefig(waveform_path, bbox_inches="tight", dpi=300)
    plt.close(fig_wave)

    # âœ… ì£¼íŒŒìˆ˜(Spectrum) ê·¸ë˜í”„ ì €ì¥
    fig_spec, ax3 = plt.subplots(figsize=(10, 3))
    ax3.plot(NELOW_fft_data, color="purple")
    ax3.set_ylim(0, max(NELOW_fft_data) * 1.3)
    fig_spec.savefig(spectrum_path, bbox_inches="tight", dpi=300)
    plt.close(fig_spec)

    return waveform_path, spectrum_path, actual_class, wave_energy, wave_max_frequency


# 3ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜ (í´ë”ì—ì„œ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì™€ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥)
def load_dataset(data_folder):
    images_1, images_2, labels = [], [], []
    class_mapping = {"leak": 0, "meter": 1, "no_leak": 2}  # í´ë˜ìŠ¤ ë§¤í•‘

    for class_name in os.listdir(data_folder):
        class_path = os.path.join(data_folder, class_name)
        if not os.path.isdir(class_path):  # í´ë”ê°€ ì•„ë‹Œ ê²½ìš° ë¬´ì‹œ
            continue

        files = sorted(os.listdir(class_path))  # ì´ë¯¸ì§€ ì •ë ¬
        for i in tqdm(range(0, len(files) - 1, 2), desc=f"ğŸ“‚ {class_name} ë¡œë”© ì¤‘", unit="pair"):  # ì´ë¯¸ì§€ 2ê°œì”© ìŒìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
            img1_path = os.path.join(class_path, files[i])
            img2_path = os.path.join(class_path, files[i + 1])

            img1, img2 = load_image_pair(img1_path, img2_path)
            images_1.append(img1)
            images_2.append(img2)
            labels.append(class_mapping[class_name])  # í´ë˜ìŠ¤ ë¼ë²¨ ì €ì¥

    images_1 = np.array(images_1)
    images_2 = np.array(images_2)
    labels = tf.keras.utils.to_categorical(labels, num_classes=CLASSES)  # ì›-í•« ì¸ì½”ë”©
    return images_1, images_2, labels
def plot_history(history):
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # Accuracy ê·¸ë˜í”„
    axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy')
    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[0, 0].set_title('Model Accuracy')
    axes[0, 0].set_xlabel('Epochs')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()

    # Loss ê·¸ë˜í”„
    axes[0, 1].plot(history.history['loss'], label='Train Loss')
    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')
    axes[0, 1].set_title('Model Loss')
    axes[0, 1].set_xlabel('Epochs')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()

    # Precision ê·¸ë˜í”„
    axes[1, 0].plot(history.history['precision_m'], label='Train Precision')
    axes[1, 0].plot(history.history['val_precision_m'], label='Validation Precision')
    axes[1, 0].set_title('Model Precision')
    axes[1, 0].set_xlabel('Epochs')
    axes[1, 0].set_ylabel('Precision')
    axes[1, 0].legend()

    # Recall & F1-score ê·¸ë˜í”„
    axes[1, 1].plot(history.history['recall_m'], label='Train Recall')
    axes[1, 1].plot(history.history['val_recall_m'], label='Validation Recall')
    axes[1, 1].plot(history.history['f1_m'], label='Train F1-score')
    axes[1, 1].plot(history.history['val_f1_m'], label='Validation F1-score')
    axes[1, 1].set_title('Recall & F1-score')
    axes[1, 1].set_xlabel('Epochs')
    axes[1, 1].set_ylabel('Score')
    axes[1, 1].legend()

    plt.tight_layout()
    # ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
    plt.savefig(train_plot, dpi=300)
    plt.show()

# ğŸ“Œ ì´ë¯¸ì§€ ì˜ˆì¸¡ ìˆ˜í–‰ í•¨ìˆ˜
def predict_images(image_path1, image_path2, model):
    CLASSES = ["leak", "meter", "no_leak"]
    img1 = cv2.imread(image_path1)
    img2 = cv2.imread(image_path2)

    img1 = cv2.resize(img1, (128, 128)) / 255.0
    img2 = cv2.resize(img2, (128, 128)) / 255.0

    img1 = np.expand_dims(img1, axis=0)
    img2 = np.expand_dims(img2, axis=0)

    predictions = model.predict([img1, img2])
    predicted_class_idx = np.argmax(predictions, axis=1)[0]
    predicted_class = CLASSES[predicted_class_idx]
    confidence = np.max(predictions) * 100

    return predicted_class, confidence


if model_training:
    # 4ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    train_folder = "C:/Users/gram/AI/NELOW/NELOW_AI/Graph_Image/train_data"  # í•™ìŠµ ë°ì´í„° í´ë”
    val_folder = "C:/Users/gram/AI/NELOW/NELOW_AI/Graph_Image/val_data"  # ê²€ì¦ ë°ì´í„° í´ë”

    x_train_1, x_train_2, y_train = load_dataset(train_folder)
    x_val_1, x_val_2, y_val = load_dataset(val_folder)

    print(f"í•™ìŠµ ë°ì´í„° í¬ê¸°: {x_train_1.shape}, {x_train_2.shape}, {y_train.shape}")
    print(f"ê²€ì¦ ë°ì´í„° í¬ê¸°: {x_val_1.shape}, {x_val_2.shape}, {y_val.shape}")

    # 5ï¸âƒ£ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    model = build_dual_input_model()
    model.summary()

    history = model.fit(
        [x_train_1, x_train_2], y_train, validation_data=([x_val_1, x_val_2], y_val), batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1
    )

    # 6ï¸âƒ£ ëª¨ë¸ ì €ì¥
    model.save(AI_model_training)
    print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
    # ê·¸ë˜í”„ ì¶œë ¥
    plot_history(history)
    # 7ï¸âƒ£ ëª¨ë¸ í‰ê°€
    results = model.evaluate([x_val_1, x_val_2], y_val)

    # âœ… í‰ê°€ ì§€í‘œ ì¶œë ¥
    metrics = ["Loss", "Accuracy", "F1 Score", "Precision", "Recall"]
    for metric, value in zip(metrics, results):
        print(f"{metric}: {value:.4f}")

if model_testing:
    # ğŸ“Œ ëª¨ë¸ ë¡œë“œ (ì»´íŒŒì¼ False ì„¤ì •)
    AI_model = tf.keras.models.load_model(AI_model_testing, compile=False)

    # ğŸ“Œ ê²°ê³¼ CSV íŒŒì¼ ì„¤ì •
    csv_file_path = os.path.join(csv_output_path, "test_results_ì‹ ì™„ì£¼_ì†Œì–‘.csv")

    # ğŸ“Œ ë°ì´í„° ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    filenames, max_amplitudes, max_frequencies = [], [], []
    real_label, AI_model_predict = [], []

    # ğŸ“Œ WAV íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (í´ë” ë‚´ ëª¨ë“  .wav íŒŒì¼)
    wav_files = [os.path.join(wav_root_dir, f) for f in os.listdir(wav_root_dir) if f.endswith('.wav')]

    if not wav_files:
        print("âš ï¸ ê²½ê³ : WAV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print(f"ğŸ“‚ ì´ {len(wav_files)} ê°œì˜ WAV íŒŒì¼ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")

    # ğŸ“Œ CSV íŒŒì¼ ì‘ì„± (ì“°ê¸° ëª¨ë“œ)
    with open(csv_file_path, mode="w", newline="", encoding="utf-8") as csv_file:
        writer = csv.writer(csv_file)
        writer.writerow(["íŒŒì¼ ì´ë¦„", "ì†Œë¦¬ ìµœëŒ€ ì§„í­", "ì†Œë¦¬ ìµœëŒ€ ì£¼íŒŒìˆ˜", "ì‹¤ì œ í´ë˜ìŠ¤", "ì˜ˆì¸¡ í´ë˜ìŠ¤"])  # CSV í—¤ë”

        # ğŸ“Œ WAV íŒŒì¼ í•˜ë‚˜ì”© ì²˜ë¦¬ (ì§„í–‰ë¥  í‘œì‹œ)
        for wav_file in tqdm(wav_files, desc="ğŸ” WAV íŒŒì¼ ì˜ˆì¸¡ ì¤‘", unit="file"):
            try:
                # âœ… ìŒì°¨íŠ¸ ë° ì£¼íŒŒìˆ˜ ì°¨íŠ¸ ìƒì„±
                waveform_path, spectrum_path, actual_class, wave_energy, wave_max_frequency = save_graphs(wav_file, output_data_dir)
                print('ìµœëŒ€ ì§„í­', wave_energy)
                print('ìµœëŒ€ ì£¼íŒŒìˆ˜', wave_max_frequency)

                # âœ… ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
                predicted_class, confidence = predict_images(waveform_path, spectrum_path, AI_model)
                print('ì˜ˆì¸¡ í´ë˜ìŠ¤', predicted_class, '======>', confidence)

                # # âœ… ìµœëŒ€ ì§„í­ ë° ìµœëŒ€ ì£¼íŒŒìˆ˜ ê°’ ê³„ì‚°
                max_amplitude = wave_energy
                max_frequency = wave_max_frequency

                # âœ… ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                filenames.append(os.path.basename(wav_file))
                max_amplitudes.append(max_amplitude)
                max_frequencies.append(max_frequency)
                real_label.append(actual_class)
                AI_model_predict.append(predicted_class)
                print('clear data_list append')
                # âœ… CSV íŒŒì¼ì— ì €ì¥
                writer.writerow(
                    [os.path.basename(wav_file), max_amplitude, max_frequency, actual_class, predicted_class])
                print('clear csv file save')
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {wav_file} - {e}")

    # ğŸ“Œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    final_df = pd.DataFrame({
        "íŒŒì¼_ì´ë¦„": filenames,
        "ì†Œë¦¬_ìµœëŒ€_ì§„í­": max_amplitudes,
        "ì†Œë¦¬_ìµœëŒ€_ì£¼íŒŒìˆ˜": max_frequencies,
        "ì‹¤ì œ_í´ë˜ìŠ¤": real_label,
        "ì˜ˆì¸¡_í´ë˜ìŠ¤": AI_model_predict
    })

    # âœ… ì •ë ¬: ì†Œë¦¬ ìµœëŒ€ ì§„í­ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    final_df = final_df.sort_values(by="ì†Œë¦¬_ìµœëŒ€_ì§„í­", ascending=False)

    # âœ… CSV íŒŒì¼ë¡œ ì €ì¥
    final_df.to_csv(csv_file_path, index=False, encoding="utf-8-sig")
    print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ CSV íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {csv_file_path}")
